{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prepare_dataset.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGixX4xBaLM7"
      },
      "source": [
        "# fliter bad matches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZhZvapMWyaC"
      },
      "source": [
        "!pip install -U -q PyDrive\r\n",
        "from pydrive.auth import GoogleAuth\r\n",
        "from pydrive.drive import GoogleDrive\r\n",
        "from google.colab import auth\r\n",
        "from oauth2client.client import GoogleCredentials\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "auth.authenticate_user()\r\n",
        "gauth = GoogleAuth()\r\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\r\n",
        "drive = GoogleDrive(gauth)\r\n",
        "download_files = [\r\n",
        "          {'id':'1n1cIwfqqUIYipKI6Gn8tVwS9XRNNYW8q','file_name':'chat.csv'},\r\n",
        "          \r\n",
        "          ]\r\n",
        "for entity in download_files:\r\n",
        "  id1 = entity['id']\r\n",
        "  downloaded = drive.CreateFile({'id':id1 }) \r\n",
        "  downloaded.GetContentFile(entity['file_name']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbZD2PVGXp2v",
        "outputId": "34c8ab7f-5f52-4e2e-e048-36355d84b945"
      },
      "source": [
        "refine_toxicity_words = []\r\n",
        "for line in open(\"refined_toxicity_lexicon.txt\"):\r\n",
        "  refine_toxicity_words.append(line[:-1])\r\n",
        "len(refine_toxicity_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1504"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfh3KvhrhgAF"
      },
      "source": [
        "refine_toxicity_words[-1] = 'noobi'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3N4IGZzXvMx",
        "outputId": "0a8d168f-f1dc-4c67-8611-d5400982ca6f"
      },
      "source": [
        "refine_toxicity_words = list(set(refine_toxicity_words))\r\n",
        "len(refine_toxicity_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1503"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kcKgMSrXweC",
        "outputId": "49da9b89-8944-4030-e2c7-d09c02e5f36b"
      },
      "source": [
        "import re\r\n",
        "def remove_punctuation(x):\r\n",
        "  x = re.sub(r\"[^a-zA-Z\\s]\",'',x)\r\n",
        "  return x\r\n",
        "\r\n",
        "df_whole_chat = pd.read_csv('chat.csv')\r\n",
        "bad_match_id1 = []\r\n",
        "for index, row in df_whole_chat.iterrows():\r\n",
        "  msg = str(row['key'])\r\n",
        "  if row['match_id'] in bad_match_id1:\r\n",
        "    continue\r\n",
        "  msg = remove_punctuation(msg)\r\n",
        "  msg = msg.lower()\r\n",
        "  for word in msg.split():\r\n",
        "    if word in refine_toxicity_words:\r\n",
        "      bad_match_id1.append(row['match_id'])\r\n",
        "      break\r\n",
        "len(bad_match_id1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30204"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "x8P8UsJ8YoT3",
        "outputId": "2249c119-c1ee-461d-d70e-1104a8f57454"
      },
      "source": [
        "matches_data_after_fliter = []\r\n",
        "for index, row in df_whole_chat.iterrows():\r\n",
        "  if row['match_id'] in bad_match_id1:\r\n",
        "    matches_data_after_fliter.append([row['match_id'],row['key'],row['slot'],row['time'],row['unit']])\r\n",
        "from pandas.core.frame import DataFrame\r\n",
        "bad_match_new_lexicon = DataFrame(matches_data_after_fliter,columns=['match_id','key','slot','time','unit'])\r\n",
        "bad_match_new_lexicon.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>match_id</th>\n",
              "      <th>key</th>\n",
              "      <th>slot</th>\n",
              "      <th>time</th>\n",
              "      <th>unit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>force it</td>\n",
              "      <td>6</td>\n",
              "      <td>-8</td>\n",
              "      <td>6k Slayer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>space created</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>Monkey</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>hah</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>Monkey</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>ez 500</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "      <td>6k Slayer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>mvp ulti</td>\n",
              "      <td>4</td>\n",
              "      <td>934</td>\n",
              "      <td>Kira</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   match_id            key  slot  time       unit\n",
              "0         0       force it     6    -8  6k Slayer\n",
              "1         0  space created     1     5     Monkey\n",
              "2         0            hah     1     6     Monkey\n",
              "3         0         ez 500     6     9  6k Slayer\n",
              "4         0       mvp ulti     4   934       Kira"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CpPoQW1Ypow",
        "outputId": "f2d3a67d-295c-4856-fc3a-85739676bb32"
      },
      "source": [
        "bad_match_new_lexicon.to_csv('refined_lexicon_flitered_match.csv')\r\n",
        "print('using new lexicon data size:',bad_match_new_lexicon.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using new lexicon data size: 1139729\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7774SiUaQKW"
      },
      "source": [
        "# generate utterance level data and word level data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y741bPa_aVFm"
      },
      "source": [
        "import re\r\n",
        "df_bad_match = pd.read_csv('refined_lexicon_flitered_match.csv')\r\n",
        "utterance_list = df_bad_match['key'].tolist()\r\n",
        "regex1 = r'[а-яА-Я]{1,}'\r\n",
        "regex1 = re.compile(regex1)\r\n",
        "regex2 = re.compile(u\"[\\u4e00-\\u9fa5]+\")\r\n",
        "new_result = []\r\n",
        "for index, row in df_bad_match.iterrows():\r\n",
        "  utter = row['key']\r\n",
        "  utter = str(utter)\r\n",
        "  if not re.search(regex1,utter) and '????' not in utter:\r\n",
        "    # print(utter)\r\n",
        "    new_result.append([row['match_id'],row['key'],row['slot'],row['time'],row['unit']])\r\n",
        "from pandas.core.frame import DataFrame\r\n",
        "df_bad_match_remove_useless = DataFrame(new_result,columns=['match_id','key','slot','time','unit'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi41LJs5abG8",
        "outputId": "0bc7d8a0-44b8-4550-a92c-a9728e86a168"
      },
      "source": [
        "df_bad_match_remove_useless.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1097463"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "aRG8p8nPao1D",
        "outputId": "590fe2df-8093-48d2-9d8c-faf7c4309c8f"
      },
      "source": [
        "df_bad_match = df_bad_match_remove_useless\r\n",
        "conversation_ids = []\r\n",
        "conversation_id = -1\r\n",
        "current_match_id = -1\r\n",
        "for index, row in df_bad_match.iterrows():\r\n",
        "  match_id = row['match_id']\r\n",
        "  if match_id == current_match_id:\r\n",
        "    lag = row['time'] - last_time\r\n",
        "    if lag >60:\r\n",
        "      conversation_id +=1\r\n",
        "    conversation_ids.append(conversation_id)\r\n",
        "    current_match_id = match_id\r\n",
        "    last_time = row['time']\r\n",
        "  else:\r\n",
        "    conversation_id +=1\r\n",
        "    conversation_ids.append(conversation_id)\r\n",
        "    last_time = row['time']\r\n",
        "    current_match_id = match_id\r\n",
        "df_bad_match_remove_useless['conversation_id'] = conversation_ids\r\n",
        "df_bad_match_remove_useless.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>match_id</th>\n",
              "      <th>key</th>\n",
              "      <th>slot</th>\n",
              "      <th>time</th>\n",
              "      <th>unit</th>\n",
              "      <th>conversation_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>force it</td>\n",
              "      <td>6</td>\n",
              "      <td>-8</td>\n",
              "      <td>6k Slayer</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>space created</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>Monkey</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>hah</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>Monkey</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>ez 500</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "      <td>6k Slayer</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>mvp ulti</td>\n",
              "      <td>4</td>\n",
              "      <td>934</td>\n",
              "      <td>Kira</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   match_id            key  slot  time       unit  conversation_id\n",
              "0         0       force it     6    -8  6k Slayer                0\n",
              "1         0  space created     1     5     Monkey                0\n",
              "2         0            hah     1     6     Monkey                0\n",
              "3         0         ez 500     6     9  6k Slayer                0\n",
              "4         0       mvp ulti     4   934       Kira                1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW6tWtAbawmn"
      },
      "source": [
        "# These are just common English contractions. There are many edge cases. i.e. University's working on it.\r\n",
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \r\n",
        "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \r\n",
        "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \r\n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \r\n",
        "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \r\n",
        "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \r\n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \r\n",
        "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \r\n",
        "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\r\n",
        "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \r\n",
        "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \r\n",
        "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\r\n",
        "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \r\n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \r\n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \r\n",
        "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \r\n",
        "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \r\n",
        "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \r\n",
        "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \r\n",
        "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \r\n",
        "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\r\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "aomhXTMVa5F8",
        "outputId": "b0241fb0-dc4d-44fd-d5f8-5b7cc2c59147"
      },
      "source": [
        "## constract restore\r\n",
        "def contraction_restore(x):\r\n",
        "  x = str(x)\r\n",
        "  new_str = ''\r\n",
        "  for word in x.split():\r\n",
        "    if word in contraction_dict:\r\n",
        "      new_str += contraction_dict[word] + ' '\r\n",
        "    else:\r\n",
        "      new_str += word + ' '\r\n",
        "  return new_str\r\n",
        "df_bad_match_remove_useless['key_after_contraction'] = df_bad_match_remove_useless['key'].map(contraction_restore)\r\n",
        "df_bad_match_remove_useless.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>match_id</th>\n",
              "      <th>key</th>\n",
              "      <th>slot</th>\n",
              "      <th>time</th>\n",
              "      <th>unit</th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>key_after_contraction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>force it</td>\n",
              "      <td>6</td>\n",
              "      <td>-8</td>\n",
              "      <td>6k Slayer</td>\n",
              "      <td>0</td>\n",
              "      <td>force it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>space created</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>Monkey</td>\n",
              "      <td>0</td>\n",
              "      <td>space created</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>hah</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>Monkey</td>\n",
              "      <td>0</td>\n",
              "      <td>hah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>ez 500</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "      <td>6k Slayer</td>\n",
              "      <td>0</td>\n",
              "      <td>ez 500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>mvp ulti</td>\n",
              "      <td>4</td>\n",
              "      <td>934</td>\n",
              "      <td>Kira</td>\n",
              "      <td>1</td>\n",
              "      <td>mvp ulti</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   match_id            key  ...  conversation_id  key_after_contraction\n",
              "0         0       force it  ...                0              force it \n",
              "1         0  space created  ...                0         space created \n",
              "2         0            hah  ...                0                   hah \n",
              "3         0         ez 500  ...                0                ez 500 \n",
              "4         0       mvp ulti  ...                1              mvp ulti \n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQggsLEWa6pF"
      },
      "source": [
        "### separate\r\n",
        "import re\r\n",
        "def separate_special_char_for_each_word(x):\r\n",
        "  regex = re.compile(r\"[a-zA-Z][^:;=a-zA-Z\\s\\t]\")\r\n",
        "  if re.search(regex,x):\r\n",
        "    result = re.search(regex,x)\r\n",
        "    a = list(x)\r\n",
        "    a.insert(result.start()+1, ' ')\r\n",
        "    x = ''.join(a)\r\n",
        "  regex = re.compile(r\"[^:;=a-zA-Z\\s\\t<][a-zA-Z]\")\r\n",
        "  if re.search(regex,x):\r\n",
        "    result = re.search(regex,x)\r\n",
        "    a = list(x)\r\n",
        "    a.insert(result.start()+1, ' ')\r\n",
        "    x = ''.join(a)\r\n",
        "  return x\r\n",
        "\r\n",
        "def separate_special_char_for_each_word_revise(x):\r\n",
        "  regex1 = re.compile(r\"[a-zA-Z][^a-zA-Z\\s\\t0-9]\")\r\n",
        "  regex2 = re.compile(r\"[^a-zA-Z\\s\\t0-9:=][a-zA-Z]\")\r\n",
        "  while re.search(regex1,x) or re.search(regex2,x):\r\n",
        "    if re.search(regex1,x):\r\n",
        "      result = re.search(regex1,x)\r\n",
        "      a = list(x)\r\n",
        "      a.insert(result.start()+1, ' ')\r\n",
        "      x = ''.join(a)\r\n",
        "    if re.search(regex2,x):\r\n",
        "      result = re.search(regex2,x)\r\n",
        "      a = list(x)\r\n",
        "      a.insert(result.start()+1, ' ')\r\n",
        "      x = ''.join(a)\r\n",
        "  return x\r\n",
        "\r\n",
        "def separate_special_char(x):\r\n",
        "  new_utterance = ''\r\n",
        "  for word in str(x).split():\r\n",
        "    word = separate_special_char_for_each_word(word)\r\n",
        "    new_utterance+=word + ' '\r\n",
        "  return new_utterance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "BfMdK1kHa-wP",
        "outputId": "f442fb30-28da-4105-a234-cd3ea725dd24"
      },
      "source": [
        "df_bad_match_remove_useless['key_after_separation'] = df_bad_match_remove_useless['key_after_contraction'].map(separate_special_char_for_each_word_revise)\r\n",
        "df_bad_match_remove_useless.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>match_id</th>\n",
              "      <th>key</th>\n",
              "      <th>slot</th>\n",
              "      <th>time</th>\n",
              "      <th>unit</th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>key_after_contraction</th>\n",
              "      <th>key_after_separation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>force it</td>\n",
              "      <td>6</td>\n",
              "      <td>-8</td>\n",
              "      <td>6k Slayer</td>\n",
              "      <td>0</td>\n",
              "      <td>force it</td>\n",
              "      <td>force it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>space created</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>Monkey</td>\n",
              "      <td>0</td>\n",
              "      <td>space created</td>\n",
              "      <td>space created</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>hah</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>Monkey</td>\n",
              "      <td>0</td>\n",
              "      <td>hah</td>\n",
              "      <td>hah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>ez 500</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "      <td>6k Slayer</td>\n",
              "      <td>0</td>\n",
              "      <td>ez 500</td>\n",
              "      <td>ez 500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>mvp ulti</td>\n",
              "      <td>4</td>\n",
              "      <td>934</td>\n",
              "      <td>Kira</td>\n",
              "      <td>1</td>\n",
              "      <td>mvp ulti</td>\n",
              "      <td>mvp ulti</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   match_id            key  ...  key_after_contraction  key_after_separation\n",
              "0         0       force it  ...              force it              force it \n",
              "1         0  space created  ...         space created         space created \n",
              "2         0            hah  ...                   hah                   hah \n",
              "3         0         ez 500  ...                ez 500                ez 500 \n",
              "4         0       mvp ulti  ...              mvp ulti              mvp ulti \n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "zE8n9p2LbBHg",
        "outputId": "b95a150f-46cb-42ea-b338-fd36a21e097b"
      },
      "source": [
        "## remove special characters\r\n",
        "regex_init = \"[^:;=][^a-zA-Z0-9\\s:=<;]\"\r\n",
        "regex_wenhao = \"[?]\"\r\n",
        "regex_chinese = re.compile(u\"[\\u4e00-\\u9fa5]+\")\r\n",
        "regex_rec = \"[]\"\r\n",
        "regex_first = \"^[^a-zA-Z0-9\\s:=<;]\"\r\n",
        "def remove_special_chars(y):\r\n",
        "  utter = str(y)\r\n",
        "  new_result = ''\r\n",
        "  for word in utter.split():\r\n",
        "    # print(word)\r\n",
        "    new_utter = re.sub(regex_init,'',word)\r\n",
        "    new_utter = re.sub(regex_wenhao,'',new_utter)\r\n",
        "    new_utter = re.sub(regex_chinese,'',new_utter)\r\n",
        "    new_utter = re.sub(regex_rec,'',new_utter)\r\n",
        "    new_utter = re.sub(regex_first,'',new_utter)\r\n",
        "    new_utter = new_utter.strip()\r\n",
        "    if new_utter != '':\r\n",
        "      new_result += new_utter+' '\r\n",
        "  return new_result\r\n",
        "df_bad_match_remove_useless['key_after_removal'] = df_bad_match_remove_useless['key_after_separation'].map(remove_special_chars)\r\n",
        "df_bad_match_remove_useless.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>match_id</th>\n",
              "      <th>key</th>\n",
              "      <th>slot</th>\n",
              "      <th>time</th>\n",
              "      <th>unit</th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>key_after_contraction</th>\n",
              "      <th>key_after_separation</th>\n",
              "      <th>key_after_removal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>force it</td>\n",
              "      <td>6</td>\n",
              "      <td>-8</td>\n",
              "      <td>6k Slayer</td>\n",
              "      <td>0</td>\n",
              "      <td>force it</td>\n",
              "      <td>force it</td>\n",
              "      <td>force it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>space created</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>Monkey</td>\n",
              "      <td>0</td>\n",
              "      <td>space created</td>\n",
              "      <td>space created</td>\n",
              "      <td>space created</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>hah</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>Monkey</td>\n",
              "      <td>0</td>\n",
              "      <td>hah</td>\n",
              "      <td>hah</td>\n",
              "      <td>hah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>ez 500</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "      <td>6k Slayer</td>\n",
              "      <td>0</td>\n",
              "      <td>ez 500</td>\n",
              "      <td>ez 500</td>\n",
              "      <td>ez 500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>mvp ulti</td>\n",
              "      <td>4</td>\n",
              "      <td>934</td>\n",
              "      <td>Kira</td>\n",
              "      <td>1</td>\n",
              "      <td>mvp ulti</td>\n",
              "      <td>mvp ulti</td>\n",
              "      <td>mvp ulti</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   match_id            key  ...  key_after_separation  key_after_removal\n",
              "0         0       force it  ...             force it           force it \n",
              "1         0  space created  ...        space created      space created \n",
              "2         0            hah  ...                  hah                hah \n",
              "3         0         ez 500  ...               ez 500             ez 500 \n",
              "4         0       mvp ulti  ...             mvp ulti           mvp ulti \n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DV6zF2QbETy"
      },
      "source": [
        "combined_utterance = []\r\n",
        "last_match_id = -1\r\n",
        "last_conversation_id = -1\r\n",
        "for index, row in df_bad_match_remove_useless.iterrows():\r\n",
        "  match_id = row['match_id']\r\n",
        "  if match_id == last_match_id and last_conversation_id == row['conversation_id'] and last_user_name == row['unit'] and last_slot == row['slot'] and row['time'] - last_time <=60:\r\n",
        "    combined_utterance[-1][2] = combined_utterance[-1][2] + ' [SEPA] '+str(row['key_after_removal'])\r\n",
        "  else:\r\n",
        "    combined_utterance.append([match_id,row['conversation_id'],str(row['key_after_removal']),row['time'],row['slot'],row['unit']])\r\n",
        "    last_match_id = match_id\r\n",
        "    last_conversation_id = row['conversation_id']\r\n",
        "    last_user_name = row['unit']\r\n",
        "    last_slot = row['slot']\r\n",
        "    last_time = row['time']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eha316aSbWY0",
        "outputId": "64e5e3c8-fe0c-4111-d98f-929ff7cabe71"
      },
      "source": [
        "from pandas.core.frame import DataFrame\r\n",
        "utterance2=DataFrame(combined_utterance,columns=['match_id','conversation_id','utterance','time','slot','unit'])\r\n",
        "word_level_utterance = utterance2['utterance'].tolist()\r\n",
        "utterance=DataFrame(combined_utterance,columns=['match_id','conversation_id','utterance','time','slot','unit'])\r\n",
        "utterance['word_level_utterance'] = word_level_utterance\r\n",
        "utterance.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "783866"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "YBhBAu2k2d3N",
        "outputId": "b99fa475-0c4a-48e4-c656-2689ac5dd50f"
      },
      "source": [
        "utterance.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>match_id</th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>utterance</th>\n",
              "      <th>time</th>\n",
              "      <th>slot</th>\n",
              "      <th>unit</th>\n",
              "      <th>word_level_utterance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>force it</td>\n",
              "      <td>-8</td>\n",
              "      <td>6</td>\n",
              "      <td>6k Slayer</td>\n",
              "      <td>force it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>space created  [SEPA] hah</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>Monkey</td>\n",
              "      <td>space created  [SEPA] hah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>ez 500</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>6k Slayer</td>\n",
              "      <td>ez 500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>mvp ulti</td>\n",
              "      <td>934</td>\n",
              "      <td>4</td>\n",
              "      <td>Kira</td>\n",
              "      <td>mvp ulti</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>bye</td>\n",
              "      <td>1486</td>\n",
              "      <td>6</td>\n",
              "      <td>6k Slayer</td>\n",
              "      <td>bye</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   match_id  conversation_id  ...       unit        word_level_utterance\n",
              "0         0                0  ...  6k Slayer                   force it \n",
              "1         0                0  ...     Monkey  space created  [SEPA] hah \n",
              "2         0                0  ...  6k Slayer                     ez 500 \n",
              "3         0                1  ...       Kira                   mvp ulti \n",
              "4         0                2  ...  6k Slayer                        bye \n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6etPrTjz4D67"
      },
      "source": [
        "# auto labelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikuy9o6U4NW3"
      },
      "source": [
        "!pip install -U -q PyDrive\r\n",
        "from pydrive.auth import GoogleAuth\r\n",
        "from pydrive.drive import GoogleDrive\r\n",
        "from google.colab import auth\r\n",
        "from oauth2client.client import GoogleCredentials\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "\r\n",
        "auth.authenticate_user()\r\n",
        "gauth = GoogleAuth()\r\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\r\n",
        "drive = GoogleDrive(gauth)\r\n",
        "download_files = [\r\n",
        "          {'id':'1VALiNA_ruYl0jt66MBG7Eb_o3sWQIBqj','file_name':'50K_Output.csv'},\r\n",
        "          {'id':'1-HsJ2w944FNCZPPiqYjV7Ip-2fmdNqz5','file_name':'word_level_data_combined_annotation.csv'},\r\n",
        "          {'id':'1vHhnY7vvA5FZemgka1-mDTE-GadI0-bq','file_name':'50K_Output_wrong_label_removed.csv'},\r\n",
        "          {'id':'1o3CQc34m1sFSZWVCwmIlVRJoE8XItdBW','file_name':'wrong_label_in_the_dataset.csv'},\r\n",
        "          {'id':'1CQTKmXpo1l9TFv4-juuyU7NN1bTsnr_b','file_name':'50K_Output2.csv'},\r\n",
        "          {'id':'1EJy2AXC7ELU-6TJd4SDbi1kNGM-4b4U-','file_name':'50K_Output_from_xinghong.csv'},\r\n",
        "          {'id':'17_RsK0dBShqjGV-NAH4W3Nt7EbdFdep6','file_name':'50K_Output_from_xinghong2.csv'},\r\n",
        "          {'id':'1y_AlSVQhqjikF3otPvI5RwrYGv_6vo0H','file_name':'45k_cleaned.csv'},\r\n",
        "          {'id':'1Jxi1j9aTXA7lg-tFy2tz_6uM40D7xUNk','file_name':'cleand_lexicon.csv'},\r\n",
        "          {'id':'1SbsUfo-p5WdlvhZsCTvbUVDVyvTnjuvo','file_name':'cleand_and_refined_lexicon.csv'},\r\n",
        "          {'id':'1TJVC6VGiAqrMzNbh_-a70FWlR6zzLuvP','file_name':'45k_add_slot_label.csv'},\r\n",
        "          ]\r\n",
        "for entity in download_files:\r\n",
        "  id1 = entity['id']\r\n",
        "  downloaded = drive.CreateFile({'id':id1 }) \r\n",
        "  downloaded.GetContentFile(entity['file_name']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Smzw7Iac4FKa",
        "outputId": "79e43a2a-62f3-4737-ef01-84e75abc5ed9"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "# colnames = ['1','2','3','utterance','4','5','6','Label','combined_annotation']\r\n",
        "lexicons = pd.read_csv(\"cleand_and_refined_lexicon.csv\")\r\n",
        "print(lexicons.head())\r\n",
        "C_lexicon = lexicons['C'].tolist()\r\n",
        "D_lexicon = lexicons['D'].tolist()\r\n",
        "P_lexicon = lexicons['P'].tolist()\r\n",
        "S_lexicon = lexicons['S'].tolist()\r\n",
        "T_lexicon = lexicons['T'].tolist()\r\n",
        "def fliter_empty(ll):\r\n",
        "  new_ll = []\r\n",
        "  for l in ll:\r\n",
        "    if str(l) != 'nan':\r\n",
        "      new_ll.append(str(l).lower())\r\n",
        "  return new_ll\r\n",
        "C_lexicon = fliter_empty(C_lexicon)\r\n",
        "D_lexicon = fliter_empty(D_lexicon)\r\n",
        "P_lexicon = fliter_empty(P_lexicon)\r\n",
        "S_lexicon = fliter_empty(S_lexicon)\r\n",
        "T_lexicon = fliter_empty(T_lexicon)\r\n",
        "print('C lexicon size:',len(C_lexicon))\r\n",
        "print('D lexicon size:',len(D_lexicon))\r\n",
        "print('P lexicon size:',len(P_lexicon))\r\n",
        "print('S lexicon size:',len(S_lexicon))\r\n",
        "print('T lexicon size:',len(T_lexicon))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Unnamed: 0       C       D        P          S      T\n",
            "0           0      aa   25mmr      all        2ez   4r5e\n",
            "1           1     aaa   abyss     alll        afk   5h1t\n",
            "2           2    aaaa      ac  allllll      aggro   5hit\n",
            "3           3   aaaaa   aeges  another  anchients    a55\n",
            "4           4  aaaaaa  aeghis      any    ancient  a_s_s\n",
            "C lexicon size: 355\n",
            "D lexicon size: 386\n",
            "P lexicon size: 172\n",
            "S lexicon size: 284\n",
            "T lexicon size: 1627\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdeEzrmv4HD8",
        "outputId": "5590b4cf-06ae-4ad7-8e3a-46c315539bc6"
      },
      "source": [
        "C_lexicon_set = set(C_lexicon)\r\n",
        "D_lexicon_set = set(D_lexicon)\r\n",
        "P_lexicon_set = set(P_lexicon)\r\n",
        "S_lexicon_set = set(S_lexicon)\r\n",
        "T_lexicon_set = set(T_lexicon)\r\n",
        "print('C lexicon size:',len(C_lexicon_set))\r\n",
        "print('D lexicon size:',len(D_lexicon_set))\r\n",
        "print('P lexicon size:',len(P_lexicon_set))\r\n",
        "print('S lexicon size:',len(S_lexicon_set))\r\n",
        "print('T lexicon size:',len(T_lexicon_set))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C lexicon size: 355\n",
            "D lexicon size: 386\n",
            "P lexicon size: 172\n",
            "S lexicon size: 284\n",
            "T lexicon size: 1627\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEJEQ4XQ4J8V"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "# colnames = ['1','2','3','utterance','4','5','6','Label','combined_annotation']\r\n",
        "data = pd.read_csv(\"45k_cleaned.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1bagwSa4ROd"
      },
      "source": [
        "import re\r\n",
        "def get_word_level_text(x):\r\n",
        "  x = str(x)\r\n",
        "  if x == 'nan':\r\n",
        "    return ''\r\n",
        "  split_array = re.split(\"\\(SEPA\\),|\\([ODPSTC]\\),\", x)\r\n",
        "  final_sentence = []\r\n",
        "  for word in split_array:\r\n",
        "    if word == '':\r\n",
        "      continue\r\n",
        "    final_sentence.append(word.strip())\r\n",
        "  return ' '.join(final_sentence)\r\n",
        "data['word_level_data'] = data['combined_annotation'].map(get_word_level_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qCQVLxk4iWa"
      },
      "source": [
        "def annotate(x):\r\n",
        "  x = str(x).lower()\r\n",
        "  if x == '':\r\n",
        "    return ''\r\n",
        "  annotations = ''\r\n",
        "  for value in x.split():\r\n",
        "    add_on = 'O '\r\n",
        "    if value == '.':\r\n",
        "      add_on='O '\r\n",
        "    if value == '[sepa]':\r\n",
        "      add_on='SEPA '\r\n",
        "    if value in T_lexicon_set:\r\n",
        "      add_on='T '\r\n",
        "    if value in S_lexicon_set:\r\n",
        "      add_on='S '\r\n",
        "    if value in D_lexicon_set:\r\n",
        "      add_on='D '\r\n",
        "    if value in P_lexicon_set:\r\n",
        "      add_on='P '\r\n",
        "    if value in C_lexicon_set:\r\n",
        "      add_on='C '\r\n",
        "    annotations+=add_on\r\n",
        "  if len(annotations.split()) != len(x.split()):\r\n",
        "    print(x)\r\n",
        "  return annotations\r\n",
        "data['slot_label'] = data['word_level_data'].map(annotate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncTUmKPh4mHk"
      },
      "source": [
        "def combine_annotation(utter,anno):\r\n",
        "  if utter == '':\r\n",
        "    return ''\r\n",
        "  result = ''\r\n",
        "  utter_list = utter.split()\r\n",
        "  anno_list = anno.split()\r\n",
        "  assert len(utter_list) == len(anno_list)\r\n",
        "  for i in range(len(utter_list)):\r\n",
        "    result += utter_list[i] + ' '+ '('+anno_list[i]+'),'+' '\r\n",
        "  return result\r\n",
        "data['combined_annotation_version2'] = data.apply(lambda row: combine_annotation(row['word_level_data'], row['slot_label']), axis=1)\r\n",
        "data = data.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])\r\n",
        "data.to_csv('45k_after_slot_annotation.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPIPkhOzGall"
      },
      "source": [
        "# construct input for baseline models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgFzpZqcGgV9"
      },
      "source": [
        "from sklearn.utils import shuffle\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "data = pd.read_csv(\"45k_add_slot_label.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_bPmw9pGmd9"
      },
      "source": [
        "import numpy as np\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train, X_test = train_test_split(data,  test_size=0.4, random_state=42)\r\n",
        "X_val, X_test= train_test_split(X_test,  test_size=0.5, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqQuU9TRGt1t"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "train_seqin=[]\r\n",
        "\r\n",
        "train_label=[]\r\n",
        "train_seqout = []\r\n",
        "\r\n",
        "train_seqin = X_train['word_level_data'].tolist()\r\n",
        "train_label= X_train['final_label'].tolist()\r\n",
        "train_seqout = X_train['slot_label'].tolist()\r\n",
        "print(len(train_seqout))\r\n",
        "print(len(train_seqin))\r\n",
        "print(len(train_label))\r\n",
        "\r\n",
        "pd.DataFrame(train_seqin).to_csv(\"./seq.in\", index=False, header=None)\r\n",
        "\r\n",
        "pd.DataFrame(train_seqout).to_csv(\"./seq.out\", index=False, header=None)\r\n",
        "pd.DataFrame(train_label).to_csv(\"./label\", index=False, header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg65S6hdGuwt"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "train_seqin=[]\r\n",
        "\r\n",
        "train_label=[]\r\n",
        "train_seqout = []\r\n",
        "\r\n",
        "train_seqin = X_val['word_level_data'].tolist()\r\n",
        "train_label= X_val['final_label'].tolist()\r\n",
        "train_seqout = X_val['slot_label'].tolist()\r\n",
        "print(len(train_seqout))\r\n",
        "print(len(train_seqin))\r\n",
        "print(len(train_label))\r\n",
        "\r\n",
        "pd.DataFrame(train_seqin).to_csv(\"./seq.in\", index=False, header=None)\r\n",
        "\r\n",
        "pd.DataFrame(train_seqout).to_csv(\"./seq.out\", index=False, header=None)\r\n",
        "pd.DataFrame(train_label).to_csv(\"./label\", index=False, header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiSwTaGbGxCd"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "train_seqin=[]\r\n",
        "\r\n",
        "train_label=[]\r\n",
        "train_seqout = []\r\n",
        "\r\n",
        "train_seqin = X_test['word_level_data'].tolist()\r\n",
        "train_label= X_test['final_label'].tolist()\r\n",
        "train_seqout = X_test['slot_label'].tolist()\r\n",
        "print(len(train_seqout))\r\n",
        "print(len(train_seqin))\r\n",
        "print(len(train_label))\r\n",
        "\r\n",
        "pd.DataFrame(train_seqin).to_csv(\"./seq.in\", index=False, header=None)\r\n",
        "\r\n",
        "pd.DataFrame(train_seqout).to_csv(\"./seq.out\", index=False, header=None)\r\n",
        "pd.DataFrame(train_label).to_csv(\"./label\", index=False, header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2kUDw_yHCim"
      },
      "source": [
        "## Bi-Model-Intent-And-Slot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELC3eAbfHFPP"
      },
      "source": [
        "from sklearn.utils import shuffle\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "data = pd.read_csv(\"45k_add_slot_label.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtKOH6hdHI71"
      },
      "source": [
        "import re\r\n",
        "p1 = re.compile(r'[(](.*?)[)]', re.S)\r\n",
        "new_lines = []\r\n",
        "for index, row in data.iterrows(): \r\n",
        "  words = str(row['word_level_data']).split()\r\n",
        "  temp = str(row['slot_label']).split()\r\n",
        "  label = row['final_label']\r\n",
        "  assert len(words) == len(temp)\r\n",
        "  new_line = []\r\n",
        "  for word, slot_label in zip(words, temp):\r\n",
        "    new_line.append(str(word).strip()+'<::>'+str(slot_label).strip())\r\n",
        "  new_line_str = ' '.join(new_line)\r\n",
        "  new_line_str += ' <=> '+str(label)\r\n",
        "  new_lines.append(new_line_str)\r\n",
        "data['new_utterance'] = new_lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj63aZFEHNF2"
      },
      "source": [
        "from sklearn.utils import shuffle\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train, X_test = train_test_split(data,  test_size=0.4, random_state=42)\r\n",
        "X_val, X_test= train_test_split(X_test,  test_size=0.5, random_state=42)\r\n",
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc7GBCPvHSDt"
      },
      "source": [
        "import pandas as pd\r\n",
        "seq = X_train['new_utterance'].tolist()\r\n",
        "pd.DataFrame(seq).to_csv(\"./train_dev\", index=False, header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3v9lwRmHU1m"
      },
      "source": [
        "import pandas as pd\r\n",
        "seq = X_test['new_utterance'].tolist()\r\n",
        "pd.DataFrame(seq).to_csv(\"./test\", index=False, header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV6Jue4uHZXl"
      },
      "source": [
        "# construct input for context model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyL7HfOfHbf_"
      },
      "source": [
        "from sklearn.utils import shuffle\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "data = pd.read_csv(\"45k_add_slot_label.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbxP9BN1Hdwu"
      },
      "source": [
        "context_data = []\r\n",
        "conversation_id_old = -1\r\n",
        "for index, row in data.iterrows():\r\n",
        "  conversation_id = row['conversation_id']\r\n",
        "  if conversation_id!=conversation_id_old:\r\n",
        "    context_data.append(row['word_level_data'])\r\n",
        "  else:\r\n",
        "    old_context = context_data[-1]\r\n",
        "    context_data.append(str(old_context)+' [eos] '+str(row['word_level_data']))\r\n",
        "  conversation_id_old = conversation_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnC_2hwQHggW"
      },
      "source": [
        "context_data_depth3 = []\r\n",
        "souce_back_depth = 7\r\n",
        "conversation_id_old = -1\r\n",
        "for context in context_data:\r\n",
        "  context = str(context)\r\n",
        "  context_split = context.split('[eos]')\r\n",
        "  if len(context_split)<=souce_back_depth:\r\n",
        "    context_data_depth3.append(context)\r\n",
        "  else:\r\n",
        "    context_split_reserve = context_split[-souce_back_depth:]\r\n",
        "    context_data_depth3.append('[eos]'.join(context_split_reserve))\r\n",
        "print(len(context_data_depth3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV1dzgN9HikW"
      },
      "source": [
        "data['context_data']=context_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ9-BdtyHktO"
      },
      "source": [
        "import numpy as np\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train, X_test = train_test_split(data,  test_size=0.4, random_state=42)\r\n",
        "X_val, X_test= train_test_split(X_test,  test_size=0.5, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgQAM2hqHmgu"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "train_seqin=[]\r\n",
        "\r\n",
        "train_label=[]\r\n",
        "train_seqout = []\r\n",
        "\r\n",
        "train_seqin = X_train['context_data'].tolist()\r\n",
        "train_label= X_train['final_label'].tolist()\r\n",
        "train_seqout = X_train['slot_label'].tolist()\r\n",
        "print(len(train_seqout))\r\n",
        "print(len(train_seqin))\r\n",
        "print(len(train_label))\r\n",
        "\r\n",
        "pd.DataFrame(train_seqin).to_csv(\"./seq.in\", index=False, header=None)\r\n",
        "\r\n",
        "pd.DataFrame(train_seqout).to_csv(\"./seq.out\", index=False, header=None)\r\n",
        "pd.DataFrame(train_label).to_csv(\"./label\", index=False, header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjFnLyDBHm7W"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "train_seqin=[]\r\n",
        "\r\n",
        "train_label=[]\r\n",
        "train_seqout = []\r\n",
        "\r\n",
        "train_seqin = X_val['context_data'].tolist()\r\n",
        "train_label= X_val['final_label'].tolist()\r\n",
        "train_seqout = X_val['slot_label'].tolist()\r\n",
        "print(len(train_seqout))\r\n",
        "print(len(train_seqin))\r\n",
        "print(len(train_label))\r\n",
        "\r\n",
        "pd.DataFrame(train_seqin).to_csv(\"./seq.in\", index=False, header=None)\r\n",
        "\r\n",
        "pd.DataFrame(train_seqout).to_csv(\"./seq.out\", index=False, header=None)\r\n",
        "pd.DataFrame(train_label).to_csv(\"./label\", index=False, header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYUy1ymyHn0G"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "train_seqin=[]\r\n",
        "\r\n",
        "train_label=[]\r\n",
        "train_seqout = []\r\n",
        "\r\n",
        "train_seqin = X_test['word_level_data'].tolist()\r\n",
        "train_label= X_test['final_label'].tolist()\r\n",
        "train_seqout = X_test['slot_label'].tolist()\r\n",
        "print(len(train_seqout))\r\n",
        "print(len(train_seqin))\r\n",
        "print(len(train_label))\r\n",
        "\r\n",
        "pd.DataFrame(train_seqin).to_csv(\"./seq.in\", index=False, header=None)\r\n",
        "\r\n",
        "pd.DataFrame(train_seqout).to_csv(\"./seq.out\", index=False, header=None)\r\n",
        "pd.DataFrame(train_label).to_csv(\"./label\", index=False, header=None)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}